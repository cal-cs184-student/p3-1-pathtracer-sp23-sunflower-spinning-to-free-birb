<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">YOUR NAME(S)</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">TODO</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>

<p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
<o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
<p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


<p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
<ul>
<li>Your main report page should be called index.html.</li>
<li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
<li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
<li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
<li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
<li>And again, test your website on the instructional machines early!</li>
</ul>


<p>Here is an example of how to include a simple formula:</p>
<p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
<p>or, alternatively, you can include an SVG image of a LaTex formula.</p>

<div>

    <h2 align="middle">Overview</h2>
    <p>
        YOUR RESPONSE GOES HERE
    </p>
    <br />

    <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
    <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
    Explain the triangle intersection algorithm you implemented in your own words.
    Show images with normal shading for a few small .dae files. -->

    <h3>
        Walk through the ray generation and primitive intersection parts of the rendering pipeline.
    </h3>
    <p>
        The ray generation and primitive intersection parts of the rendering pipeline are two key stages in the process of generating an image from a 3D scene using a raytracer.
    </p>

    <p>
        At a high level, the ray generation stage involves generating a set of rays that are cast from the camera through the pixels of the image. Each ray represents a straight line from the camera position through a pixel and into the 3D scene. The primitive intersection stage involves computing the intersection of each ray with the primitives in the scene, such as spheres or triangles, in order to determine what objects in the scene are visible from the camera viewpoint.
    </p>

    <p>
        We fill in the function <b>Camera::generate_ray(...)</b>, which takes the normalized image coordinates (x,y) as input and outputs a Ray in the world space and <b>PathTracer::raytrace_pixel(...)</b>
        , which takes pixel coordinates(x,y) as input and updates the corresponding pixel.
    </p>

    <p>
        For the former function, we transform the image coordinates to camera space,generate the ray in the camera space then finally transform it into a ray in the world space, using variables including hFov, vFov, pos, nClip, fClip.
    </p>

    <p>
        For the latter function, we generate ns_aa random rays using generate_ray(...) implemented in part 1. The integral of radiance over this pixel is estimated by averaging ns_aa samples.
    </p>
    <ul>
        <li>
            <b>Task 1-Ray Generation:</b>


            </p><p>
                Task 1 can be divided into 3 steps:
            </p>

            <p>
                transform the image coordinates to camera space -> generate the ray in the camera space -> a ray in the world space
            </p>


            </p>

            <p>
                The formulas we use for computing position of the input sensor sample coordinate on the canonical sensor plane one unit away from the pinhole is as follows:
            </p>

            <img src="images/eA1.svg" /> <br />
            <img src="images/eA2.svg" /> <br />

            <img src="images/eA3.svg" /> <br />
            <img src="images/eA4.svg" /> <br />
            <img src="images/eA5.svg" /> <br />

            <p>
                <b>Variables used for implementation:</b> <br />
                define the sensor: hFov,vFov <br />
                camera position in the world place:pos <br />
                the camera-to-world rotation matrix:c2w <br />
                define the visible edge of the camera:nclip&fclip <br />
            </p>




        </li>
        <li>
            <b>Task 2-Scene Intersection</b>:
            <p>
                This function takes in the pixel coordinates (x, y) and updates the corresponding pixel in the sampleBuffer with an estimate of the integral of radiance over this pixel. The estimate is obtained by averaging ns_aa samples.
            </p>

            <p>
                To estimate the integral of radiance over a pixel, we need to generate ns_aa random rays and trace them through the scene using the est_radiance_global_illumination function. The resulting radiance values are then accumulated and averaged to get the final estimate.
            </p>

            <p>
                In the loop, we generate a random ray through the pixel using camera->generate_ray and trace it through the scene using est_radiance_global_illumination. The resulting radiance value is accumulated in the variable L_out. After ns_aa samples have been taken, we average the radiance values by dividing L_out by ns_aa. Finally, we update the corresponding pixel in the sampleBuffer with the final estimate using sampleBuffer.update_pixel.
            </p>

        </li>

    </ul>
    </p>
    <br />

    <h3>
        Explain the triangle intersection algorithm you implemented in your own words.
    </h3>
    <p>
        The Möller–Trumbore intersection algorithm we implemented is a method for efficiently testing if a ray intersects with a triangle in 3D space.
    </p>
    <br />

    <p>
        The ray is defined by an origin point O and a direction vector <b>v</b>. Every point on the ray can be expressed by \begin{aligned}t=O+tv\end{aligned}, where the parameter t ranges from zero to infinity. The triangle is defined by three vertices.
    </p>

    <p>
        Using barycentric coordinates, any point on the triangle can be expressed as a convex combination of the triangle's verticies: P=wv<sub>1</sub>+uv<sub>2</sub>+vv<sub>3</sub>
    </p>

    <p>
        The coefficients must be non-negative and sum to 1, so w can be replaced with 1-u-v:
    </p>

    <p>
        \begin{aligned}P&=(1-u-v)v_{1}+uv_{2}+vv_{3}\\P&=v_{1}+u(v_{2}-v_{1})+v(v_{3}-v_{1})\end{aligned}
        , where P is any point on the plane.
    </p>



    <p>
        To find u and v for a particular intersection, set the ray expression equal to the plane expression, and put the variables on one side and the constants on the other.
    </p>

    <p>
        \begin{aligned}O+tD&=v_{1}+u(v_{2}-v_{1})+v(v_{3}-v_{1})\\O-v_{1}&=-tD+u(v_{2}-v_{1})+v(v_{3}-v_{1})\end{aligned}
    </p>

    <p>
        This is a system of linear equations with three equations (one each for  x, y, z) and three unknowns (t, u, v), and can be represented as a matrix-vector multiplication.

    </p>

    <img src="images/eAX.svg" /> <br />


    <p>
        To perform the intersection test, the algorithm first computes the edge vectors of the triangle and the vector from the ray origin to one of the triangle's vertices. Then it computes the normal vector of the triangle as the cross product of two of its edge vectors.Next, the algorithm calculates the determinant of the matrix formed by the ray direction and the edge vectors of the triangle. If this determinant is close to zero, then the ray and the triangle are parallel and there is no intersection. If the determinant is non-zero, the algorithm continues by calculating the inverse of the determinant and using it to compute the parameters of the barycentric coordinate system of the intersection point.Finally, the algorithm checks whether the barycentric coordinates of the intersection point are within the range of [0,1], indicating that the intersection point lies within the triangle. If so, the algorithm returns the distance of the intersection point along the ray direction as well as the barycentric coordinates of the intersection point.Overall, the Möller–Trumbore algorithm is a simple and efficient method for testing ray-triangle intersections, making it a popular choice for use in real-time rendering applications.

    </p>

    <h3>
        Show images with normal shading for a few small .dae files.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/test_banana.png" align="middle" width="400px" />

                </td>
                <td>
                    <img src="images/test_building.png" align="middle" width="400px" />

                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/test_cube.png" align="middle" width="400px" />

                </td>
                <td>
                    <img src="images/test_plane.png" align="middle" width="400px" />

                </td>
            </tr>
        </table>
    </div>
    <br />


    <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
    <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
    Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

    <h3>
        Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    </h3>
    </p><p>
        YOUR RESPONSE GOES HERE
    </p>

    <h3>
        Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example1.dae</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example2.dae</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example3.dae</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example4.dae</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />

    <h3>
        Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
    </h3>
    <p>
        YOUR RESPONSE GOES HERE
    </p>
    <br />

    <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
    <!-- Walk through both implementations of the direct lighting function.
    Show some images rendered with both implementations of the direct lighting function.
    Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
    Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

    <h3>
        Walk through both implementations of the direct lighting function.
    </h3>
    <p>
        YOUR RESPONSE GOES HERE
    </p>

    <h3>
        Show some images rendered with both implementations of the direct lighting function.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <!-- Header -->
            <tr align="center">
                <th>
                    <b>Uniform Hemisphere Sampling</b>
                </th>
                <th>
                    <b>Light Sampling</b>
                </th>
            </tr>
            <br />
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example1.dae</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example1.dae</figcaption>
                </td>
            </tr>
            <br />
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example2.dae</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example2.dae</figcaption>
                </td>
            </tr>
            <br />
        </table>
    </div>
    <br />

    <h3>
        Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="200px" />
                    <figcaption>1 Light Ray (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="200px" />
                    <figcaption>4 Light Rays (example1.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="200px" />
                    <figcaption>16 Light Rays (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="200px" />
                    <figcaption>64 Light Rays (example1.dae)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <p>
        YOUR EXPLANATION GOES HERE
    </p>
    <br />

    <h3>
        Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
    </h3>
    <p>
        YOUR RESPONSE GOES HERE
    </p>
    <br />


    <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
    <!-- Walk through your implementation of the indirect lighting function.
    Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
    For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
    Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

    <h3>
        Walk through your implementation of the indirect lighting function.
    </h3>
    <p>
        YOUR RESPONSE GOES HERE
    </p>
    <br />

    <h3>
        Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example1.dae</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>example2.dae</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />

    <h3>
        Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Only direct illumination (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Only indirect illumination (example1.dae)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />
    <p>
        YOUR EXPLANATION GOES HERE
    </p>
    <br />

    <h3>
        For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />
    <p>
        YOUR EXPLANATION GOES HERE
    </p>
    <br />

    <h3>
        Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>1 sample per pixel (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>2 samples per pixel (example1.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>4 samples per pixel (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>8 samples per pixel (example1.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>16 samples per pixel (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>64 samples per pixel (example1.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>1024 samples per pixel (example1.dae)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />
    <p>
        YOUR EXPLANATION GOES HERE
    </p>
    <br />


    <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
    <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
    Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

    <h3>
        Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
    </h3>
    <p>
        YOUR RESPONSE GOES HERE
    </p>
    <br />

    <h3>
        Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
        <table style="width:100%">
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Rendered image (example1.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Sample rate image (example1.dae)</figcaption>
                </td>
            </tr>
            <tr align="center">
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Rendered image (example2.dae)</figcaption>
                </td>
                <td>
                    <img src="images/your_file.png" align="middle" width="400px" />
                    <figcaption>Sample rate image (example2.dae)</figcaption>
                </td>
            </tr>
        </table>
    </div>
    <br />


</div></body>
</html>
